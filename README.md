# Deepfake Video Detection using Visual and Audio Features

This project is a web-based tool for detecting deepfake videos using both **visual** and **audio (NLP)** features. It uses **XGBoost** as the main model and supports other optional models like Logistic Regression and Random Forest for comparison.

## ğŸŒŸ Features

- ğŸ¥ Upload any video file (.mp4)
- ğŸ“Š Extracts frame-level visual features (color, texture, motion)
- ğŸ§ Extracts audio features using NLP-like methods (MFCC, chroma, spectral)
- ğŸ¤– Predicts if the video is FAKE or REAL using XGBoost
- âœ… Displays prediction confidence, extracted features, and visual output
- ğŸ–¥ï¸ Clean and responsive frontend UI

---

## ğŸ§  How It Works

### ğŸ“Œ 1. Video Processing
- Videos are uploaded from the frontend and sent to the FastAPI backend.
- Frames are extracted using OpenCV.
- Audio is extracted using `librosa`.

### ğŸ“Œ 2. Feature Extraction

#### Visual Features:
- Mean and variance of RGB channels
- Edge density using Canny
- Texture entropy using Laplacian
- Motion consistency & smoothness

#### Audio Features (NLP-inspired):
- **MFCCs (Mel-frequency cepstral coefficients)**
- **Spectral centroid**
- **Zero-crossing rate**
- **Chroma features**
- **RMS (Root Mean Square) energy**

These features simulate speech-based consistency and stability often seen in deepfakes.

### ğŸ“Œ 3. Classification (Model)
- Trained using `XGBoost`, a powerful gradient boosting algorithm
- Also supports Logistic Regression and Random Forest for experimentation
- Output includes:
  - Prediction (`REAL` or `FAKE`)
  - Confidence score
  - Feature list

---

## ğŸ“ Project Structure

deepfake-detection/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                  # FastAPI backend
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ xgboost_model.pkl    # Trained XGBoost model
â”‚   â”‚   â””â”€â”€ train_model.py       # Training script
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ extract_visual.py    # Extract visual features
â”‚   â”‚   â””â”€â”€ extract_audio.py     # Extract NLP audio features
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html               # UI
â”‚   â”œâ”€â”€ style.css                # Styling
â”‚   â””â”€â”€ script.js                # Logic for upload and results
â”œâ”€â”€ train_sample_model/
â”‚   â”œâ”€â”€ metadata.json            # Labels for training data
â”‚   â””â”€â”€ video files              # Deepfake and real videos

---

## ğŸ› ï¸ Requirements

### Python Packages (backend)

Install with:

```bash
pip install -r requirements.txt
```

#### `requirements.txt`

fastapi
uvicorn
xgboost
scikit-learn
opencv-python
numpy
pandas
librosa
pydub
python-multipart

> Ensure `ffmpeg` is installed for audio extraction:

```bash
sudo apt install ffmpeg
```

---

## ğŸš€ Run the Project

### Backend (FastAPI)

```bash
cd backend
uvicorn main:app --reload
```

### Frontend

Just open `frontend/index.html` in a browser (you can also serve via a static server).

---

## ğŸ§ª Training the Model

To retrain the model:

```bash
python backend/model/train_model.py
```

This script reads labeled videos from `train_sample_model/`, extracts visual and audio features, and trains the XGBoost model (`xgboost_model.pkl`).

---

## ğŸ§  Models Used

- âœ… **XGBoost**: Primary classifier
- ğŸ” Logistic Regression & Random Forest (optional comparison)
- ğŸ§ **NLP-inspired audio features** via `librosa`

---

## ğŸ“¸ Sample Output

- **Prediction**: REAL
- **Confidence**: 92.85%
- **Extracted Features**: Displayed in a clean grid UI
- **Color-coded**: REAL (green), FAKE (red)

---

## ğŸ“š Credits

Created by Ashwin Sujesh  
[GitHub](https://github.com/Ash2004win) | [LinkedIn](https://www.linkedin.com/in/ashwin-sujesh)
